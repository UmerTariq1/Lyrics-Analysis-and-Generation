{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1-IljxBhH8Dx7zRrI8Zrej-G9EuepeJyq","authorship_tag":"ABX9TyNWjxKzgxaelL55nFz11pln"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYJNe4aEloes","executionInfo":{"status":"ok","timestamp":1693993951224,"user_tz":-300,"elapsed":20363,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"9a8c8f87-9542-482b-f360-3458df6ac133"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}],"source":["!pip install -q transformers einops accelerate langchain bitsandbytes\n","!pip install sentencepiece"]},{"cell_type":"code","source":["#My huggingFace credentials:\n","# \n","!huggingface-cli login\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xN7q4vE4lqK9","outputId":"d836db12-8c01-47d8-e088-7d9f44ab6fef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: "]}]},{"cell_type":"code","source":["from langchain import PromptTemplate,  LLMChain\n","from langchain import HuggingFacePipeline\n","from transformers import AutoTokenizer\n","import transformers\n","import torch\n","import re\n","import pandas as pd\n","import time\n","from tqdm import tqdm"],"metadata":{"id":"aCdL3IzAl8g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title clean_text\n","\n","def clean_text(text, remove_punct = False, remove_new_line = True):\n","    '''\n","    given a string of text, remove brackets, (remove puntuations, optional) and remove escape sequences\n","\n","    @params:\n","        text : string\n","    @returns:\n","        string which is cleaned\n","    '''\n","\n","    clean_text = re.sub(r'\\([^)]*\\)', '', text).strip()\n","    clean_text = re.sub(r'\\{[^)]*\\}', '', clean_text).strip()\n","    clean_text = re.sub(r'\\[[^)]*\\]', '', clean_text).strip()\n","\n","    #remove punctuations using string.punctuation\n","    if remove_punct:\n","        clean_text = clean_text.translate(str.maketrans('', '', string.punctuation.replace(\"-\", \"\")))\n","\n","    #remove escape sequences\n","    if remove_new_line:\n","      clean_text = clean_text.replace('\\n', ' ')\n","    clean_text = clean_text.replace('\\t', ' ')\n","    clean_text = clean_text.replace('\\r', ' ')\n","    clean_text = clean_text.replace('\\x0c', ' ')\n","\n","    #remove extra spaces\n","    clean_text = re.sub(' +', ' ', clean_text).strip()\n","\n","    return clean_text"],"metadata":{"cellView":"form","id":"TW1K8_HAnFyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title model declaration\n","model = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\", #task\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    max_length=1000,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id\n",")"],"metadata":{"cellView":"form","id":"l45u2gdbmcgW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"],"metadata":{"id":"Xel00jqQmceE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = \"/content/drive/MyDrive/MusicProject/data_clean.csv\"\n","OUTPUT_PATH = \"/content/drive/MyDrive/MusicProject/notebooks/output/\"\n","\n","df = pd.read_csv(DATA_PATH)"],"metadata":{"id":"woKzfqBEst8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.shape)\n","df_en = df[df['language'] == 'en']\n","print(df_en.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QisJS2Best6P","executionInfo":{"status":"ok","timestamp":1693722099824,"user_tz":-300,"elapsed":979,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"b0f841e9-9c12-4c93-95e5-3c38e469a9fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(189866, 11)\n","(129306, 11)\n"]}]},{"cell_type":"code","source":["sample_song = df_en[(df_en['artist'] == 'Taylor Swift') & (df_en['title'] == 'I Knew You Were Trouble.')]\n","sample_song_lyrics = clean_text(sample_song[\"lyrics_clean\"].values[0] , remove_new_line = False)\n","sample_song_title = sample_song[\"title\"].values[0]\n","sample_song_artist = sample_song[\"artist\"].values[0]\n","\n","sample_params = {\n","    \"singer_name\" : sample_song_artist,\n","    \"song_name\" : sample_song_title,\n","    \"song_lyrics\" : sample_song_lyrics,\n","}\n"],"metadata":{"id":"b3XM2CLGxpJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pop_famous_artists = [\"Taylor Swift\", \"Selena Gomez\", \"Dua Lipa\", \"Ariana Grande\", \"Justin Bieber\", \"Billie Eilish\",\n","                      \"Lady Gaga\", \"Adele\" , \"Britney Spears\", \"Katy Perry\" , \"Ed Sheeran\", \"Rihanna\", \"Bruno Mars\",\n","                      \"Demi Lovato\", \"Miley Cyrus\", \"Jennifer Lopez\", \"Charlie Puth\"]\n","\n","# get the rows in which the artist is in the list above and tag is pop\n","# df_pop_famous = df_en[(df_en['artist'].isin(pop_famous_artists)) & (df_en['tag'] == 'pop')]\n","# df_pop_famous.shape\n","\n","df_pop_not_famous = df_en[(df_en['tag'] == 'pop') & (~df_en['artist'].isin(pop_famous_artists))]\n","df_pop_not_famous.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2P3anLJvw4S","executionInfo":{"status":"ok","timestamp":1693723281875,"user_tz":-300,"elapsed":464,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"b515667b-9164-4d76-b551-eacbde2238a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32825, 11)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["template = \"\"\"\n","              You are a music analyst. You analyze the song lyrics and then write what the song is about.\n","              In this particular case, write the desciption of the song ```{song_name}``` by ```{singer_name}```\n","              The output description should be around 150 words.\n","              Do not mention the name of artist or the name of the song in the output description. Just directly explain what the song is about.\n","\n","              The lyrics of the song are:\n","              ############## Lyrics start ##############\n","              ```{song_lyrics}```\n","              ############## Lyrics end ##############\n","\n","              The output description should be around 150 words.\n","              Do not mention the name of artist or the name of the song in the output description. Just directly explain what the song is about.\n","\n","              Description:\n","              \"\"\"\n","\n","input_variables = [\"singer_name\", \"song_name\", \"song_lyrics\"]\n","\n","prompt = PromptTemplate( template = template, input_variables = input_variables)\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)"],"metadata":{"id":"orSR-l-Hmcbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# iterate through df_en and generate the description for each song using the model llm_chain.run\n","# use params depending on the row that is being iterated\n","# create a new dataframe and store the description in it along with all the other columns in the original dataframe\n","# create a batch size of 20 and keep saving the dataframe in the file after every batch\n","\n","new_df = pd.DataFrame(columns = df_pop_not_famous.columns.tolist() + ['description'])\n","batch_size = 20\n","batch_number = 1\n","total_time_taken = 0\n","actual_iteration_number = 0\n","# iterate through the whole dataframe df_en\n","for i in tqdm( range( 0, df_pop_not_famous.shape[0] ) ):\n","    # if i <= 1360 : #total batches done * 20\n","    #   batch_number = 69\n","    #   # check if the df_en.iloc[i]['artist'] contains one of the pop_famous_artists\n","        # if any( j in df_en.iloc[i]['artist'] for j in pop_famous_artists ):\n","          # print(\"\\n found one\")\n","          # print(\"i = \", i)\n","    #   continue\n","\n","\n","    if (i+1) % batch_size == 0:\n","        print(\"\")\n","        print(\"Average time till now is \", total_time_taken / (actual_iteration_number + 1))\n","        # print(\"saving file for batch number : \", str(batch_number))\n","        new_df.to_csv(OUTPUT_PATH + str(\"data_clean_en_with_description_llma2_\") + str(batch_number)+ \"_popNotFamous\" + str(\".csv\"))\n","        # new_df.to_csv(\"data/temp.csv\")\n","        new_df = pd.DataFrame(columns = df_pop_not_famous.columns.tolist() + ['description'])\n","        batch_number += 1\n","\n","    else:\n","        start_time = time.time()\n","        # create params dictionary using the columns artist, title, lyrics_clean\n","        params = {\n","            \"singer_name\" : df_pop_not_famous.iloc[i]['artist'],\n","            \"song_name\" : df_pop_not_famous.iloc[i]['title'],\n","            \"song_lyrics\" : df_pop_not_famous.iloc[i]['lyrics_clean']\n","        }\n","        model_output_description = llm_chain.run(params)\n","        model_output_description = clean_text (model_output_description)\n","\n","        # generate the description using llm_chain.run(params)\n","        # model_output_description = llm_chain.run(params)\n","        # model_output_description= \" description of the song\"\n","\n","        # add the description to the new_df along with all the other columns\n","        # new_df = new_df.append(df_en.iloc[i].append(pd.Series([model_output_description], index=['description'])), ignore_index=True)\n","        row_data = df_pop_not_famous.iloc[i].to_dict()\n","        row_data['description'] = model_output_description\n","        new_df = pd.concat([new_df, pd.DataFrame(row_data, index=[0])], ignore_index=True)\n","\n","\n","        end_time = time.time()\n","        total_time_taken += (end_time - start_time)\n","    actual_iteration_number += 1\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EGciZW8zt3Wb","executionInfo":{"status":"error","timestamp":1693724413391,"user_tz":-300,"elapsed":475419,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"2729c688-d7aa-4eda-f579-acf163ba385f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 6/32825 [01:12<113:06:42, 12.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1058, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 7/32825 [01:17<93:47:46, 10.29s/it] /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1008, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 8/32825 [01:23<79:57:14,  8.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1146, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 9/32825 [01:29<72:58:22,  8.01s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 10/32825 [01:41<84:27:48,  9.27s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 11/32825 [01:57<102:47:06, 11.28s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 12/32825 [02:21<137:29:03, 15.08s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 13/32825 [02:29<117:38:31, 12.91s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 14/32825 [02:38<108:03:46, 11.86s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 15/32825 [02:52<112:28:36, 12.34s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 16/32825 [03:07<120:18:24, 13.20s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 17/32825 [03:22<124:18:13, 13.64s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1012, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 18/32825 [03:27<102:15:50, 11.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 19/32825 [03:38<99:34:39, 10.93s/it] "]},{"output_type":"stream","name":"stdout","text":["\n","Average time till now is  10.89818263053894\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 20/32825 [03:38<71:48:33,  7.88s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 21/32825 [04:16<152:47:18, 16.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 22/32825 [04:30<146:54:59, 16.12s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 23/32825 [04:43<137:43:40, 15.12s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 24/32825 [04:58<137:16:41, 15.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 25/32825 [05:13<137:35:07, 15.10s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 26/32825 [05:28<135:21:31, 14.86s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 27/32825 [05:56<172:16:22, 18.91s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1064, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 28/32825 [06:02<136:58:19, 15.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1101, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 29/32825 [06:08<113:10:11, 12.42s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 30/32825 [06:21<114:34:30, 12.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1079, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 31/32825 [06:27<96:40:32, 10.61s/it] /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 32/32825 [06:34<87:16:58,  9.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 33/32825 [06:49<101:48:19, 11.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 34/32825 [07:02<106:10:23, 11.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 35/32825 [07:12<100:07:08, 10.99s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1077, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n","  0%|          | 36/32825 [07:18<86:30:53,  9.50s/it] /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 37/32825 [07:29<91:20:14, 10.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 38/32825 [07:42<99:31:21, 10.93s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  0%|          | 39/32825 [07:54<104:11:12, 11.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1511, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Average time till now is  11.852512294054032\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 40/32825 [07:55<108:12:04, 11.88s/it]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-e733601fcbc2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;34m\"song_lyrics\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdf_pop_not_famous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lyrics_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         }\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mmodel_output_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mmodel_output_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_output_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             outputs = (\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    485\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             ]\n\u001b[0;32m--> 621\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    622\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             output = (\n\u001b[0;32m--> 510\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    511\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             text = (\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     ) -> str:\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# Text generation return includes the starter text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             )\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1643\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2725\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    810\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    695\u001b[0m                 )\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    698\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 0; 14.75 GiB total capacity; 13.33 GiB already allocated; 178.81 MiB free; 13.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["x = llm_chain.run(params)"],"metadata":{"id":"LH3HcMGlsalx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_text(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116},"id":"ONQbXpM1uMaW","executionInfo":{"status":"ok","timestamp":1693722628202,"user_tz":-300,"elapsed":12,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"c76d332e-c67a-4659-f829-391d907880bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The song is about a toxic relationship where the singer realizes that the person they are involved with is trouble from the beginning. They describe how they were drawn in by the other person\\'s charm and how they ignored the red flags. The singer reflects on how they were blinded by their feelings and how they were manipulated by the other person. They express regret for getting involved with someone who never truly cared for them. The song\\'s chorus repeats the line \"I knew you were trouble when you walked in\" emphasizing the singer\\'s realization that they should have seen the warning signs earlier. Overall, the song is about the painful lessons learned from a toxic relationship and the regret that comes with getting involved with someone who is not good for you.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":[],"metadata":{"id":"WnQZb-8MuMUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fcg8VNQmvqce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dEi0ElEcvqZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W9HlVEODvqWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"\"\"\n","They told him, \"Don't you ever come around here\n","Don't wanna see your face, you better disappear\"\n","The fire's in their eyes and their words are really clear\n","So beat it, just beat it (Ooh!)\n","You better run, you better do what you can (Ooh!)\n","Don't wanna see no blood, don't be a macho man (Ooh!)\n","You wanna be tough, better do what you can\n","So beat it, but you wanna be bad\n","\n","Just beat it, beat it, beat it, beat it\n","No one wants to be defeated\n","Showin' how funky and strong is your fight\n","It doesn't matter who's wrong or right\n","Just beat it (Beat it)\n","Just beat it (Beat it)\n","Just beat it (Beat it)\n","Just beat it (Beat it, uh)\n","\n","They're out to get you, better leave while you can\n","Don't wanna be a boy, you wanna be a man\n","You wanna stay alive, better do what you can\n","So beat it, just beat it (Ooh!)\n","You have to show them that you're really not scared (Ooh!)\n","You're playin' with your life, this ain't no truth or dare (Ooh!)\n","They'll kick you, then they'll beat you, then they'll tell you it's fair\n","So beat it, but you wanna be bad\n","\n","Just beat it, beat it, beat it, beat it\n","No one wants to be defeated\n","Showin' how funky and strong is your fight\n","It doesn't matter who's wrong or right\n","Just beat it, beat it, beat it, beat it\n","No one wants to be defeated\n","Showin' how funky and strong is your fight\n","It doesn't matter who's wrong or right\n","Just beat it (Beat it, beat it, beat it)\n","Beat it (Beat it, beat it, ha, ha, ha, ha)\n","Beat it (Beat it, beat it)\n","Beat it (Beat it, beat it)\n","\n","Beat it, beat it, beat it, beat it\n","No one wants to be defeated\n","Showin' how funky and strong is your fight\n","It doesn't matter who's wrong or right (Who's right)\n","Just beat it, beat it, beat it, beat it (Hoo-hoo!)\n","No one wants to be defeated (Oh, lord)\n","Showin' how funky (Hee-hee!) and strong is your fight (Hee-hee-hee!)\n","It doesn't matter who's wrong or right\n","Just beat it, beat it, beat it, beat it (Beat it!)\n","No one wants to be defeated (Oh-no!)\n","Showin' how funky (Hoo-hoo!) and strong is your fight (Hee-hee! Hoo!)\n","It doesn't matter who's wrong or right\n","Just beat it, beat it, beat it, beat it\n","No one wants to be defeated\n","Showin' how funky and strong is your fight\n","It doesn't matter who's wrong or right (Who's right)\n","Just beat it, beat it, beat it, beat it (Hoo-hoo!)\n","No one wants to be defeated\n","\"\"\"\n","\n","x = llm_chain.run(text)"],"metadata":{"id":"NOiMrzacmcZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_text(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"id":"QQVrd7wpn7xf","executionInfo":{"status":"ok","timestamp":1693721814463,"user_tz":-300,"elapsed":12,"user":{"displayName":"Umer Tariq","userId":"13846252573045846001"}},"outputId":"5e6979d6-709f-4df3-f386-9dd07437e317"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The song \"Beat It\" by Michael Jackson is about standing up for oneself in the face of adversity. The lyrics describe a situation where someone is being bullied and told to leave, but the person refuses to back down. The song encourages the listener to show their strength and fight back against their oppressors. The lyrics also emphasize the importance of not being defeated and to keep on fighting.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"lwBZdzvjr99_"},"execution_count":null,"outputs":[]}]}
